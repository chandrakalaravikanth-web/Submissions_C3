{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6JjArX2sOhb"
      },
      "source": [
        "# Assignment 3b: Advanced Gradio RAG Frontend\n",
        "## Day 6 Session 2 - Building Configurable RAG Applications\n",
        "\n",
        "In this assignment, you'll extend your basic RAG interface with advanced configuration options to create a professional, feature-rich RAG application.\n",
        "\n",
        "**New Features to Add:**\n",
        "- Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "- Temperature slider (0 to 1 with 0.1 intervals)\n",
        "- Chunk size configuration\n",
        "- Chunk overlap configuration  \n",
        "- Similarity top-k slider\n",
        "- Node postprocessor multiselect\n",
        "- Similarity cutoff slider\n",
        "- Response synthesizer multiselect\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Advanced Gradio components and interactions\n",
        "- Dynamic RAG configuration\n",
        "- Professional UI design patterns\n",
        "- Parameter validation and handling\n",
        "- Building production-ready AI applications\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Assignment 3a (Basic Gradio RAG)\n",
        "- Understanding of RAG parameters and their effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxUjo-4XsOhc"
      },
      "source": [
        "## ðŸ“š Part 1: Setup and Imports\n",
        "\n",
        "Import all necessary libraries including advanced RAG components for configuration options.\n",
        "\n",
        "**Note:** This assignment uses OpenRouter for LLM access (not OpenAI). Make sure you have your `OPENROUTER_API_KEY` environment variable set.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EHH2Btz6tWKg",
        "outputId": "a2db8c83-e44a-4973-a715-b52a9b18a04a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 1)) (4.13.5)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 2)) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 3)) (2.187.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 4)) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 6)) (5.50.0)\n",
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 8)) (0.36.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 9)) (6.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 10)) (7.34.0)\n",
            "Collecting lancedb (from -r /requirements.txt (line 11))\n",
            "  Downloading lancedb-0.25.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting llama-index (from -r /requirements.txt (line 12))\n",
            "  Downloading llama_index-0.14.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-vector-stores-lancedb (from -r /requirements.txt (line 13))\n",
            "  Downloading llama_index_vector_stores_lancedb-0.4.2-py3-none-any.whl.metadata (460 bytes)\n",
            "Collecting llama-index-embeddings-huggingface (from -r /requirements.txt (line 14))\n",
            "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
            "Collecting llama-index-llms-huggingface-api (from -r /requirements.txt (line 15))\n",
            "  Downloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-index-embeddings-openai (from -r /requirements.txt (line 16))\n",
            "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-llms-openrouter (from -r /requirements.txt (line 17))\n",
            "  Downloading llama_index_llms_openrouter-0.4.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting llama-index-llms-openai (from -r /requirements.txt (line 18))\n",
            "  Downloading llama_index_llms_openai-0.6.11-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 19)) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 20)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 21)) (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 22)) (2.9.0)\n",
            "Collecting openai-whisper (from -r /requirements.txt (line 23))\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 24)) (2.12.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 25)) (5.1.2)\n",
            "Collecting yt-dlp (from -r /requirements.txt (line 26))\n",
            "  Downloading yt_dlp-2025.12.8-py3-none-any.whl.metadata (180 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from -r /requirements.txt (line 27)) (3.8.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r /requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r /requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /requirements.txt (line 2)) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /requirements.txt (line 2)) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /requirements.txt (line 2)) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->-r /requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->-r /requirements.txt (line 3)) (0.31.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->-r /requirements.txt (line 3)) (4.2.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /requirements.txt (line 4)) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth->-r /requirements.txt (line 4)) (4.9.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r /requirements.txt (line 6)) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio_client->-r /requirements.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client->-r /requirements.txt (line 7)) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /requirements.txt (line 8)) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r /requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r /requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r /requirements.txt (line 10))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->-r /requirements.txt (line 10)) (4.9.0)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.12/dist-packages (from lancedb->-r /requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=16 in /usr/local/lib/python3.12/dist-packages (from lancedb->-r /requirements.txt (line 11)) (18.1.0)\n",
            "Collecting lance-namespace>=0.0.16 (from lancedb->-r /requirements.txt (line 11))\n",
            "  Downloading lance_namespace-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.15.0,>=0.14.10 (from llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_core-0.14.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_readers_file-0.5.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pylance (from llama-index-vector-stores-lancedb->-r /requirements.txt (line 13))\n",
            "  Downloading pylance-1.0.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tantivy (from llama-index-vector-stores-lancedb->-r /requirements.txt (line 13))\n",
            "  Downloading tantivy-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (336 bytes)\n",
            "Collecting llama-index-llms-openai-like<0.6,>=0.5.0 (from llama-index-llms-openrouter->-r /requirements.txt (line 17))\n",
            "  Downloading llama_index_llms_openai_like-0.5.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r /requirements.txt (line 19)) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r /requirements.txt (line 19)) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->-r /requirements.txt (line 19)) (2025.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /requirements.txt (line 21)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /requirements.txt (line 21)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /requirements.txt (line 21)) (2025.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r /requirements.txt (line 22)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r /requirements.txt (line 22)) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->-r /requirements.txt (line 22)) (1.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /requirements.txt (line 23)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /requirements.txt (line 23)) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /requirements.txt (line 23)) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r /requirements.txt (line 23)) (2.9.0+cpu)\n",
            "Collecting triton>=2 (from openai-whisper->-r /requirements.txt (line 23))\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /requirements.txt (line 24)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /requirements.txt (line 24)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r /requirements.txt (line 24)) (0.4.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /requirements.txt (line 25)) (4.57.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /requirements.txt (line 25)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r /requirements.txt (line 25)) (1.16.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /requirements.txt (line 27)) (0.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r /requirements.txt (line 6)) (3.11)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->-r /requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r /requirements.txt (line 6)) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r /requirements.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r /requirements.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (3.13.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->-r /requirements.txt (line 10)) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r /requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r /requirements.txt (line 9)) (5.9.1)\n",
            "Collecting lance-namespace-urllib3-client (from lance-namespace>=0.0.16->lancedb->-r /requirements.txt (line 11))\n",
            "  Downloading lance_namespace_urllib3_client-0.3.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (0.21.0)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_workflows-2.11.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (3.6.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (4.5.1)\n",
            "Collecting setuptools>=18.5 (from ipython->-r /requirements.txt (line 10))\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (9.1.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (2.0.1)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /requirements.txt (line 12)) (0.7.1)\n",
            "Collecting pypdf<7,>=6.1.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading pypdf-6.4.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.88-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->-r /requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r /requirements.txt (line 10)) (0.2.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth->-r /requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r /requirements.txt (line 21)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->-r /requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->-r /requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /requirements.txt (line 27)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /requirements.txt (line 27)) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r /requirements.txt (line 23)) (1.14.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /requirements.txt (line 25)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /requirements.txt (line 25)) (0.7.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r /requirements.txt (line 6)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r /requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /requirements.txt (line 27)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /requirements.txt (line 27)) (7.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper->-r /requirements.txt (line 23)) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r /requirements.txt (line 25)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r /requirements.txt (line 14)) (1.22.0)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-cloud-services>=0.6.88 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.88-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r /requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12)) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper->-r /requirements.txt (line 23)) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.87-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.87 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.87-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.86-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.86 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.86-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.85-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.85 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.85-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.84-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.84 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.84-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.83-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.82 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.83-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.82-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.82-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.81-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.81 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.81-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.80-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.80 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.80-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.79-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.79 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.79-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.78-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.78 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.78-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r /requirements.txt (line 6)) (0.1.2)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r /requirements.txt (line 12))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading lancedb-0.25.3-cp39-abi3-manylinux_2_28_x86_64.whl (39.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.14.10-py3-none-any.whl (7.5 kB)\n",
            "Downloading llama_index_vector_stores_lancedb-0.4.2-py3-none-any.whl (7.9 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
            "Downloading llama_index_llms_huggingface_api-0.6.1-py3-none-any.whl (7.5 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_openrouter-0.4.2-py3-none-any.whl (4.5 kB)\n",
            "Downloading llama_index_llms_openai-0.6.11-py3-none-any.whl (26 kB)\n",
            "Downloading yt_dlp-2025.12.8-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lance_namespace-0.3.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.14.10-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai_like-0.5.3-py3-none-any.whl (4.7 kB)\n",
            "Downloading llama_index_readers_file-0.5.5-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-1.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (55.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tantivy-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-2.11.5-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.4.1-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading lance_namespace_urllib3_client-0.3.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.8/256.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=63c7f125999e202b0e4f77c2449370606e3af865289280d7dff9a111094dceb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, yt-dlp, wrapt, triton, tantivy, setuptools, pypdf, mypy-extensions, marshmallow, jedi, colorama, typing-inspect, griffe, deprecated, openai-whisper, llama-index-instrumentation, llama-cloud, lance-namespace-urllib3-client, dataclasses-json, banks, llama-index-workflows, lance-namespace, pylance, llama-index-core, lancedb, llama-index-vector-stores-lancedb, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-huggingface-api, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-huggingface, llama-cloud-services, llama-parse, llama-index-llms-openai-like, llama-index-cli, llama-index-readers-llama-parse, llama-index-llms-openrouter, llama-index\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "Successfully installed banks-2.2.0 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.15.0 jedi-0.19.2 lance-namespace-0.3.1 lance-namespace-urllib3-client-0.3.1 lancedb-0.25.3 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.10 llama-index-cli-0.5.3 llama-index-core-0.14.10 llama-index-embeddings-huggingface-0.6.1 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-huggingface-api-0.6.1 llama-index-llms-openai-0.6.11 llama-index-llms-openai-like-0.5.3 llama-index-llms-openrouter-0.4.2 llama-index-readers-file-0.5.5 llama-index-readers-llama-parse-0.5.1 llama-index-vector-stores-lancedb-0.4.2 llama-index-workflows-2.11.5 llama-parse-0.6.54 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-whisper-20250625 pylance-1.0.0 pypdf-6.4.1 setuptools-80.9.0 striprtf-0.0.26 tantivy-0.25.1 triton-3.5.1 typing-inspect-0.9.0 wrapt-1.17.3 yt-dlp-2025.12.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "5c86a1a2078245eb86b0f681f9e514c1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahm9cJxvsOhd",
        "outputId": "33b35331-a62f-42ba-fcbb-20a30f33c0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "# Advanced RAG components\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmMvXFTfsOhe"
      },
      "source": [
        "## ðŸ¤– Part 2: Advanced RAG Backend Class\n",
        "\n",
        "Create an advanced RAG backend that supports dynamic configuration of all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxWEwasnsOhe",
        "outputId": "896d1544-c594-4912-9977-2bb275530526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Advanced RAG Backend initialized and ready!\n"
          ]
        }
      ],
      "source": [
        "class AdvancedRAGBackend:\n",
        "    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.available_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "        self.available_postprocessors = [\"SimilarityPostprocessor\"]\n",
        "        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "        self.update_settings()\n",
        "\n",
        "    def update_settings(self, model: str = \"gpt-4o-mini\", temperature: float = 0.1, chunk_size: int = 512, chunk_overlap: int = 50):\n",
        "        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n",
        "        # Set up the LLM using OpenRouter\n",
        "        #api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "        api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        if api_key:\n",
        "            Settings.llm = OpenRouter(\n",
        "                api_key=api_key,\n",
        "                model=model,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "        # Set up the embedding model (keep this constant)\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set chunking parameters from function parameters\n",
        "        Settings.chunk_size = chunk_size\n",
        "        Settings.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def initialize_database(self, data_folder=\"/content/data\"):\n",
        "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
        "        # Check if data folder exists\n",
        "        if not Path(data_folder).exists():\n",
        "            return f\"âŒ Data folder '{data_folder}' not found!\"\n",
        "\n",
        "        try:\n",
        "            # Create vector store\n",
        "            vector_store = LanceDBVectorStore(\n",
        "                uri=\"./advanced_rag_vectordb\",\n",
        "                table_name=\"documents\"\n",
        "            )\n",
        "\n",
        "            # Load documents\n",
        "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "            documents = reader.load_data()\n",
        "\n",
        "            # Create storage context and index\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                show_progress=True\n",
        "            )\n",
        "\n",
        "            return f\"âœ… Database initialized successfully with {len(documents)} documents!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error initializing database: {str(e)}\"\n",
        "\n",
        "    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n",
        "        \"\"\"Get the selected postprocessor.\"\"\"\n",
        "        if postprocessor_name == \"SimilarityPostprocessor\":\n",
        "            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
        "        elif postprocessor_name == \"None\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_synthesizer(self, synthesizer_name: str):\n",
        "        \"\"\"Get the selected response synthesizer.\"\"\"\n",
        "        if synthesizer_name == \"TreeSummarize\":\n",
        "            return TreeSummarize()\n",
        "        elif synthesizer_name == \"Refine\":\n",
        "            return Refine()\n",
        "        elif synthesizer_name == \"CompactAndRefine\":\n",
        "            return CompactAndRefine()\n",
        "        elif synthesizer_name == \"Default\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def advanced_query(self, question: str, model: str, temperature: float,\n",
        "                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n",
        "                      postprocessor_names: List[str], similarity_cutoff: float,\n",
        "                      synthesizer_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n",
        "\n",
        "        # Check if index exists\n",
        "        if self.index is None:\n",
        "            return {\"response\": \"âŒ Please initialize the database first!\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "        # Check if question is empty\n",
        "        if not question or not question.strip():\n",
        "            return {\"response\": \"âš ï¸ Please enter a question first!\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "        try:\n",
        "            # Update settings with new parameters\n",
        "            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n",
        "\n",
        "            # Get postprocessors\n",
        "            postprocessors = []\n",
        "            for name in postprocessor_names:\n",
        "                processor = self.get_postprocessor(name, similarity_cutoff)\n",
        "                if processor is not None:\n",
        "                    postprocessors.append(processor)\n",
        "\n",
        "            # Get synthesizer\n",
        "            synthesizer = self.get_synthesizer(synthesizer_name)\n",
        "\n",
        "            # Create query engine with all parameters\n",
        "            query_engine_kwargs = {\"similarity_top_k\": similarity_top_k}\n",
        "            if postprocessors:\n",
        "                query_engine_kwargs[\"node_postprocessors\"] = postprocessors\n",
        "            if synthesizer is not None:\n",
        "                query_engine_kwargs[\"response_synthesizer\"] = synthesizer\n",
        "\n",
        "            query_engine = self.index.as_query_engine(**query_engine_kwargs)\n",
        "\n",
        "            # Query and get response\n",
        "            response = query_engine.query(question)\n",
        "\n",
        "            # Extract source information if available\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    sources.append({\n",
        "                        \"text\": node.text[:200] + \"...\",\n",
        "                        \"score\": getattr(node, 'score', 0.0),\n",
        "                        \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')\n",
        "                    })\n",
        "\n",
        "            return {\n",
        "                \"response\": str(response),\n",
        "                \"sources\": sources,\n",
        "                \"config\": {\n",
        "                    \"model\": model,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"chunk_size\": chunk_size,\n",
        "                    \"chunk_overlap\": chunk_overlap,\n",
        "                    \"similarity_top_k\": similarity_top_k,\n",
        "                    \"postprocessors\": postprocessor_names,\n",
        "                    \"similarity_cutoff\": similarity_cutoff,\n",
        "                    \"synthesizer\": synthesizer_name\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"response\": f\"âŒ Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "# Initialize the backend\n",
        "rag_backend = AdvancedRAGBackend()\n",
        "print(\"ðŸš€ Advanced RAG Backend initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEFPtApHsOhe"
      },
      "source": [
        "## ðŸŽ¨ Part 3: Advanced Gradio Interface\n",
        "\n",
        "Create a sophisticated Gradio interface with all the configuration options specified:\n",
        "1. Database initialization button\n",
        "2. Search query input and button  \n",
        "3. Model selection dropdown\n",
        "4. Temperature slider\n",
        "5. Chunk size and overlap inputs\n",
        "6. Similarity top-k slider\n",
        "7. Node postprocessor multiselect\n",
        "8. Similarity cutoff slider\n",
        "9. Response synthesizer multiselect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8z948bnsOhe",
        "outputId": "7218d7f4-3e53-4608-86e1-a26599eb73a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Advanced RAG interface created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_advanced_rag_interface():\n",
        "    \"\"\"Create advanced RAG interface with full configuration options.\"\"\"\n",
        "\n",
        "    def initialize_db():\n",
        "        \"\"\"Handle database initialization.\"\"\"\n",
        "        return rag_backend.initialize_database()\n",
        "\n",
        "    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap,\n",
        "                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n",
        "        \"\"\"Handle advanced RAG queries with all configuration options.\"\"\"\n",
        "        result = rag_backend.advanced_query(\n",
        "            question, model, temperature, chunk_size, chunk_overlap,\n",
        "            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n",
        "        )\n",
        "\n",
        "        # Format configuration for display\n",
        "        config_text = f\"\"\"**Current Configuration:**\\n- Model: {result['config'].get('model', 'N/A')}\\n\n",
        "        - Temperature: {result['config'].get('temperature', 'N/A')}\\n\n",
        "        - Chunk Size: {result['config'].get('chunk_size', 'N/A')}\\n\n",
        "        - Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\\n\n",
        "        - Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\\n\n",
        "        - Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\\n\n",
        "        - Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\\n\n",
        "        - Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n",
        "\n",
        "        return result[\"response\"], config_text\n",
        "\n",
        "    # TODO: Create the advanced interface structure\n",
        "    # Hint: This interface needs more complex layout with configuration controls\n",
        "\n",
        "    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n",
        "        # TODO: Add title and description\n",
        "        # Hint: Use gr.Markdown() for formatted text\n",
        "\n",
        "        # Your title and description here:\n",
        "        gr.Markdown(\"# Advanced RAG Frontend\\n\\nThis application allows you to interact with a RAG system and configure various parameters to observe their effects on the responses.\")\n",
        "\n",
        "        # TODO: Add database initialization section\n",
        "        # Hint: Use gr.Button() for initialization and gr.Textbox() for status\n",
        "        init_btn = gr.Button(\"Initialize Vector Database\")\n",
        "        status_output = gr.Textbox(label=\"Database Status\", interactive=False)\n",
        "\n",
        "\n",
        "        # TODO: Create main layout with columns\n",
        "        # Hint: Configuration controls on left, query/response on right makes sense\n",
        "        # Use gr.Row() and gr.Column() to organize this\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "\n",
        "                gr.Markdown(\"### âš™ï¸ RAG Configuration\")\n",
        "\n",
        "                # TODO: Model selection\n",
        "                # Hint: Use gr.Dropdown() with choices=[\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "                model_dropdown = gr.Dropdown(choices=rag_backend.available_models, value=\"gpt-4o-mini\", label=\"LLM Model\")\n",
        "\n",
        "\n",
        "                # TODO: Temperature control\n",
        "                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.1\n",
        "                temperature_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.1, label=\"Temperature\")\n",
        "\n",
        "\n",
        "                # TODO: Chunking parameters\n",
        "                # Hint: Use gr.Number() for numeric inputs with default values\n",
        "                chunk_size_input = gr.Number(value=512, label=\"Chunk Size\")\n",
        "\n",
        "                chunk_overlap_input = gr.Number(value=50, label=\"Chunk Overlap\")\n",
        "\n",
        "\n",
        "                # TODO: Retrieval parameters\n",
        "                # Hint: Use gr.Slider() with minimum=1, maximum=20, step=1, value=5\n",
        "                similarity_topk_slider = gr.Slider(minimum=1, maximum=20, step=1, value=5, label=\"Similarity Top-K\")\n",
        "\n",
        "\n",
        "                # TODO: Postprocessor selection\n",
        "                # Hint: Use gr.CheckboxGroup() with choices=[\"SimilarityPostprocessor\"]\n",
        "                postprocessor_checkbox = gr.CheckboxGroup(choices=rag_backend.available_postprocessors, value=[], label=\"Node Postprocessors\")\n",
        "\n",
        "\n",
        "                # TODO: Similarity filtering\n",
        "                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.3\n",
        "                similarity_cutoff_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.3, label=\"Similarity Cutoff\")\n",
        "\n",
        "\n",
        "                # TODO: Response synthesizer\n",
        "                # Hint: Use gr.Dropdown() with choices=[\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "                synthesizer_dropdown = gr.Dropdown(choices=rag_backend.available_synthesizers, value=\"Default\", label=\"Response Synthesizer\")\n",
        "\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### ðŸ’¬ Query Interface\")\n",
        "\n",
        "                # TODO: Query input\n",
        "                # Hint: Use gr.Textbox() with label=\"Ask a question\", placeholder text, lines=3\n",
        "                query_input = gr.Textbox(label=\"Ask a question\", placeholder=\"Type your question here, e.g., 'What is RAG?'\", lines=3)\n",
        "\n",
        "\n",
        "                # TODO: Submit button\n",
        "                # Hint: Use gr.Button() with variant=\"primary\"\n",
        "                submit_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
        "\n",
        "\n",
        "                # TODO: Response output\n",
        "                # Hint: Use gr.Textbox() with lines=12, interactive=False\n",
        "                response_output = gr.Textbox(label=\"Response\", lines=12, interactive=False)\n",
        "\n",
        "\n",
        "                # TODO: Configuration display\n",
        "                # Hint: Use gr.Textbox() with lines=8, interactive=False\n",
        "                config_display = gr.Textbox(label=\"Configuration Used\", lines=8, interactive=False)\n",
        "\n",
        "\n",
        "        # Uncomment to Connect functions to components\n",
        "        init_btn.click(initialize_db, outputs=[status_output])\n",
        "\n",
        "        submit_btn.click(\n",
        "            handle_advanced_query,\n",
        "            inputs=[\n",
        "                query_input, model_dropdown, temperature_slider,\n",
        "                chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n",
        "                postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n",
        "            ],\n",
        "            outputs=[response_output, config_display]\n",
        "        )\n",
        "\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "advanced_interface = create_advanced_rag_interface()\n",
        "print(\"âœ… Advanced RAG interface created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oniv1bqjsOhf"
      },
      "source": [
        "## ðŸš€ Part 4: Launch Your Advanced Application\n",
        "\n",
        "Launch your advanced Gradio application and test all the configuration options!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bQrATQgjsOhf",
        "outputId": "a46fccd9-dc2e-405d-8df2-85eed3cd266f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰ Launching your Advanced RAG Assistant...\n",
            "ðŸ”— Your application will open in a new browser tab!\n",
            "\n",
            "âš ï¸  Make sure your OPENROUTER_API_KEY environment variable is set!\n",
            "\n",
            "ðŸ“‹ Testing Instructions:\n",
            "1. Click 'Initialize Vector Database' button first\n",
            "2. Wait for success message\n",
            "3. Configure your RAG parameters:\n",
            "   - Choose model (gpt-4o, gpt-4o-mini)\n",
            "   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\n",
            "   - Set chunk size and overlap\n",
            "   - Choose similarity top-k\n",
            "   - Select postprocessors and synthesizer\n",
            "4. Enter a question and click 'Ask Question'\n",
            "5. Review both the response and configuration used\n",
            "\n",
            "ðŸ§ª Experiments to try:\n",
            "- Compare different models with the same question\n",
            "- Test temperature effects (0.1 vs 0.9)\n",
            "- Try different chunk sizes (256 vs 1024)\n",
            "- Compare synthesizers (TreeSummarize vs Refine)\n",
            "- Adjust similarity cutoff to filter results\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://27802947a61b8779fb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://27802947a61b8779fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(\"ðŸŽ‰ Launching your Advanced RAG Assistant...\")\n",
        "print(\"ðŸ”— Your application will open in a new browser tab!\")\n",
        "print(\"\")\n",
        "print(\"âš ï¸  Make sure your OPENROUTER_API_KEY environment variable is set!\")\n",
        "print(\"\")\n",
        "print(\"ðŸ“‹ Testing Instructions:\")\n",
        "print(\"1. Click 'Initialize Vector Database' button first\")\n",
        "print(\"2. Wait for success message\")\n",
        "print(\"3. Configure your RAG parameters:\")\n",
        "print(\"   - Choose model (gpt-4o, gpt-4o-mini)\")\n",
        "print(\"   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\")\n",
        "print(\"   - Set chunk size and overlap\")\n",
        "print(\"   - Choose similarity top-k\")\n",
        "print(\"   - Select postprocessors and synthesizer\")\n",
        "print(\"4. Enter a question and click 'Ask Question'\")\n",
        "print(\"5. Review both the response and configuration used\")\n",
        "print(\"\")\n",
        "print(\"ðŸ§ª Experiments to try:\")\n",
        "print(\"- Compare different models with the same question\")\n",
        "print(\"- Test temperature effects (0.1 vs 0.9)\")\n",
        "print(\"- Try different chunk sizes (256 vs 1024)\")\n",
        "print(\"- Compare synthesizers (TreeSummarize vs Refine)\")\n",
        "print(\"- Adjust similarity cutoff to filter results\")\n",
        "\n",
        "# Your code here:\n",
        "advanced_interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYoZhHfYsOhf"
      },
      "source": [
        "## ðŸ’¡ Understanding the Configuration Options\n",
        "\n",
        "### Model Selection\n",
        "- **gpt-4o**: Latest and most capable model, best quality responses\n",
        "- **gpt-4o-mini**: Faster and cheaper while maintaining good quality\n",
        "\n",
        "### Temperature (0.0 - 1.0)\n",
        "- **0.0-0.3**: Deterministic, factual responses\n",
        "- **0.4-0.7**: Balanced creativity and accuracy\n",
        "- **0.8-1.0**: More creative and varied responses\n",
        "\n",
        "### Chunk Size & Overlap\n",
        "- **Chunk Size**: How much text to process at once (256-1024 typical)\n",
        "- **Chunk Overlap**: Overlap between chunks to maintain context (10-100 typical)\n",
        "\n",
        "### Similarity Top-K (1-20)\n",
        "- **Lower values (3-5)**: More focused, faster responses\n",
        "- **Higher values (8-15)**: More comprehensive, detailed responses\n",
        "\n",
        "### Node Postprocessors\n",
        "- **SimilarityPostprocessor**: Filters out low-relevance documents\n",
        "\n",
        "### Similarity Cutoff (0.0-1.0)\n",
        "- **0.1-0.3**: More permissive, includes potentially relevant docs\n",
        "- **0.5-0.8**: More strict, only highly relevant docs\n",
        "\n",
        "### Response Synthesizers\n",
        "- **TreeSummarize**: Hierarchical summarization, good for complex topics\n",
        "- **Refine**: Iterative refinement, builds detailed responses\n",
        "- **CompactAndRefine**: Efficient version of Refine\n",
        "- **Default**: Standard synthesis approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN2Rm5nCsOhf"
      },
      "source": [
        "## âœ… Assignment Completion Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [ ] Set up your OPENROUTER_API_KEY environment variable\n",
        "- [ ] Imported all necessary libraries including advanced RAG components\n",
        "- [ ] Created AdvancedRAGBackend class with configurable parameters\n",
        "- [ ] Implemented all required methods:\n",
        "  - [ ] `update_settings()` - Updates LLM and chunking parameters\n",
        "  - [ ] `initialize_database()` - Sets up vector database\n",
        "  - [ ] `get_postprocessor()` - Returns selected postprocessor\n",
        "  - [ ] `get_synthesizer()` - Returns selected synthesizer\n",
        "  - [ ] `advanced_query()` - Handles queries with all configuration options\n",
        "- [ ] Created advanced Gradio interface with all required components:\n",
        "  - [ ] Initialize database button\n",
        "  - [ ] Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "  - [ ] Temperature slider (0 to 1, step 0.1)\n",
        "  - [ ] Chunk size input (default 512)\n",
        "  - [ ] Chunk overlap input (default 50)\n",
        "  - [ ] Similarity top-k slider (1 to 20, default 5)\n",
        "  - [ ] Node postprocessor multiselect\n",
        "  - [ ] Similarity cutoff slider (0.0 to 1.0, step 0.1, default 0.3)\n",
        "  - [ ] Response synthesizer dropdown\n",
        "  - [ ] Query input and submit button\n",
        "  - [ ] Response output\n",
        "  - [ ] Configuration display\n",
        "- [ ] Connected all components to backend functions\n",
        "- [ ] Successfully launched the application\n",
        "- [ ] Tested different parameter combinations\n",
        "- [ ] Verified all configuration options work correctly\n",
        "\n",
        "## ðŸŽŠ Congratulations!\n",
        "\n",
        "You've successfully built a professional, production-ready RAG application! You now have:\n",
        "\n",
        "- **Advanced Parameter Control**: Full control over all RAG system parameters\n",
        "- **Professional UI**: Clean, organized interface with proper layout\n",
        "- **Real-time Configuration**: Ability to experiment with different settings\n",
        "- **Production Patterns**: Understanding of how to build scalable AI applications\n",
        "\n",
        "## ðŸš€ Next Steps & Extensions\n",
        "\n",
        "**Potential Enhancements:**\n",
        "1. **Authentication**: Add user login and session management\n",
        "2. **Document Upload**: Allow users to upload their own documents\n",
        "3. **Chat History**: Implement conversation memory\n",
        "4. **Performance Monitoring**: Add response time and quality metrics\n",
        "5. **A/B Testing**: Compare different configurations side-by-side\n",
        "6. **Export Features**: Download responses and configurations\n",
        "7. **Advanced Visualizations**: Show document similarity scores and retrieval paths\n",
        "\n",
        "**Deployment Options:**\n",
        "- **Local**: Run on your machine for development\n",
        "- **Gradio Cloud**: Deploy with `interface.launch(share=True)`\n",
        "- **Hugging Face Spaces**: Deploy to Hugging Face for public access\n",
        "- **Docker**: Containerize for scalable deployment\n",
        "- **Cloud Platforms**: Deploy to AWS, GCP, or Azure\n",
        "\n",
        "You're now ready to build sophisticated AI-powered applications!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}